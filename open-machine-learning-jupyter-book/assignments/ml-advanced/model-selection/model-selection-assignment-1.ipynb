{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_regression\n\nfrom sklearn.metrics import r2_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-06T17:26:06.150338Z","iopub.execute_input":"2022-01-06T17:26:06.150821Z","iopub.status.idle":"2022-01-06T17:26:07.330284Z","shell.execute_reply.started":"2022-01-06T17:26:06.150670Z","shell.execute_reply":"2022-01-06T17:26:07.329295Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Common Regression Class","metadata":{}},{"cell_type":"markdown","source":"The most of things common in Lasson and Ridge Regression. <br/>\nThe only different between two regression is, what regularization it is using. <br/>\n- l1 regularization ---> Lasso Regression\n- l2 regularization ---> Ridge Regression","metadata":{}},{"cell_type":"code","source":"class Regression:\n    def __init__(self, learning_rate, iteration, regularization):\n        \"\"\"\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.m = None\n        self.n = None\n        self.w = None\n        self.b = None\n        self.regularization = regularization # will be the l1/l2 regularization class according to the regression model.\n        self.lr = learning_rate\n        self.it = iteration\n\n    def cost_function(self, y, y_pred):\n        \"\"\"\n        :param y: Original target value.\n        :param y_pred: predicted target value.\n        \"\"\"\n        return (1 / (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n    \n    def hypothesis(self, weights, bias, X):\n        \"\"\"\n        :param weights: parameter value weight.\n        :param X: Training samples.\n        \"\"\"\n        return np.dot(X, weights) #+ bias\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        X = np.insert(X, 0, 1, axis=1)\n\n        # Target value should be in the shape of (n, 1) not (n, ).\n        # So, this will check that and change the shape to (n, 1), if not.\n        try:\n            y.shape[1]\n        except IndexError as e:\n            # we need to change it to the 1 D array, not a list.\n            print(\"ERROR: Target array should be a one dimentional array not a list\"\n                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n            return \n        \n        # m is the number of training samples.\n        self.m = X.shape[0]\n        # n is the number of features.\n        self.n = X.shape[1]\n\n        # Set the initial weight.\n        self.w = np.zeros((self.n , 1))\n\n        # bias.\n        self.b = 0\n\n        for it in range(1, self.it+1):\n            # 1. Find the predicted value through the hypothesis.\n            # 2. Find the Cost function value.\n            # 3. Find the derivation of weights.\n            # 4. Apply Gradient Decent.\n            y_pred = self.hypothesis(self.w, self.b, X)\n            #print(\"iteration\",it)\n            #print(\"y predict value\",y_pred)\n            cost = self.cost_function(y, y_pred)\n            #print(\"Cost function\",cost)\n            # fin the derivative.\n            dw = (1/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n            #print(\"weights derivation\",dw)\n            #db = -(2 / self.m) * np.sum((y_pred - y))\n\n            # change the weight parameter.\n            self.w = self.w - self.lr * dw\n            #print(\"updated weights\",self.w)\n            #self.b = self.b - self.lr * db\n\n\n            if it % 10 == 0:\n                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n    def predict(self, test_X):\n        \"\"\"\n        :param test_X: feature values to predict.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        test_X = np.insert(test_X, 0, 1, axis=1)\n\n        y_pred = self.hypothesis(self.w, self.b, test_X)\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.332469Z","iopub.execute_input":"2022-01-06T17:26:07.332791Z","iopub.status.idle":"2022-01-06T17:26:07.346895Z","shell.execute_reply.started":"2022-01-06T17:26:07.332748Z","shell.execute_reply":"2022-01-06T17:26:07.346034Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Regularization Classes","metadata":{}},{"cell_type":"code","source":"# Create the regularization class we want.\nclass l1_regularization:\n    \"\"\"Regularization used for Lasson Regression\"\"\"\n    def __init__(self, lamda):\n        self.lamda = lamda\n\n    def __call__(self, weights):\n        \"This will be retuned when we call this class.\"\n        return self.lamda * np.sum(np.abs(weights))\n    \n    def derivation(self, weights):\n        \"Derivation of the regulariozation function.\"\n        return self.lamda * np.sign(weights)\n\n\nclass l2_regularization:\n    \"\"\"Regularization used for Ridge Regression\"\"\"\n    def __init__(self, lamda):\n        self.lamda = lamda\n\n    def __call__(self, weights):\n        \"This will be retuned when we call this class.\"\n        return self.lamda * np.sum(np.square(weights))\n    \n    def derivation(self, weights):\n        \"Derivation of the regulariozation function.\"\n        return self.lamda * 2 * (weights)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.347965Z","iopub.execute_input":"2022-01-06T17:26:07.348816Z","iopub.status.idle":"2022-01-06T17:26:07.365987Z","shell.execute_reply.started":"2022-01-06T17:26:07.348780Z","shell.execute_reply":"2022-01-06T17:26:07.364904Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data Creation","metadata":{}},{"cell_type":"code","source":"# Define the traning data.\nX, y = make_regression(n_samples=50000, n_features=8)\n\n# Chnage the shape of the target to 1 dimentional array.\ny = y[:, np.newaxis]\n\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\nprint(\"Shape of the target value ----------> {}\".format(y.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.368210Z","iopub.execute_input":"2022-01-06T17:26:07.368794Z","iopub.status.idle":"2022-01-06T17:26:07.437963Z","shell.execute_reply.started":"2022-01-06T17:26:07.368747Z","shell.execute_reply":"2022-01-06T17:26:07.436880Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"====================================================================================================\nNumber of training data samples-----> 50000\nNumber of training features --------> 8\nShape of the target value ----------> (50000, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# display the data.\ndata = pd.DataFrame(X)\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.439884Z","iopub.execute_input":"2022-01-06T17:26:07.440680Z","iopub.status.idle":"2022-01-06T17:26:07.475198Z","shell.execute_reply.started":"2022-01-06T17:26:07.440621Z","shell.execute_reply":"2022-01-06T17:26:07.474173Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          0         1         2         3         4         5         6  \\\n0 -0.633831  0.218497  1.411377  0.780343 -0.407875  0.897151 -0.096590   \n1 -0.189063  0.462887  0.716539 -0.667410 -0.365183  0.499274  0.385966   \n2 -0.521226  0.078422 -0.189474 -0.316874 -1.405221  0.049593  0.524406   \n3  0.323253  0.313048 -0.771509 -1.395240  0.545979 -1.127486 -0.455162   \n4  0.293139 -1.242824  0.165630 -0.830431  0.534965 -0.822837 -0.381272   \n\n          7  \n0  0.646323  \n1  1.318002  \n2 -0.268321  \n3  1.266323  \n4 -0.582199  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.633831</td>\n      <td>0.218497</td>\n      <td>1.411377</td>\n      <td>0.780343</td>\n      <td>-0.407875</td>\n      <td>0.897151</td>\n      <td>-0.096590</td>\n      <td>0.646323</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.189063</td>\n      <td>0.462887</td>\n      <td>0.716539</td>\n      <td>-0.667410</td>\n      <td>-0.365183</td>\n      <td>0.499274</td>\n      <td>0.385966</td>\n      <td>1.318002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.521226</td>\n      <td>0.078422</td>\n      <td>-0.189474</td>\n      <td>-0.316874</td>\n      <td>-1.405221</td>\n      <td>0.049593</td>\n      <td>0.524406</td>\n      <td>-0.268321</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.323253</td>\n      <td>0.313048</td>\n      <td>-0.771509</td>\n      <td>-1.395240</td>\n      <td>0.545979</td>\n      <td>-1.127486</td>\n      <td>-0.455162</td>\n      <td>1.266323</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.293139</td>\n      <td>-1.242824</td>\n      <td>0.165630</td>\n      <td>-0.830431</td>\n      <td>0.534965</td>\n      <td>-0.822837</td>\n      <td>-0.381272</td>\n      <td>-0.582199</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# display the data.\ndata_y = pd.DataFrame(y)\ndata_y.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.477480Z","iopub.execute_input":"2022-01-06T17:26:07.478199Z","iopub.status.idle":"2022-01-06T17:26:07.491656Z","shell.execute_reply.started":"2022-01-06T17:26:07.478149Z","shell.execute_reply":"2022-01-06T17:26:07.490437Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"            0\n0  180.847637\n1  151.465004\n2 -164.933305\n3  -73.948584\n4 -230.472983","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>180.847637</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>151.465004</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-164.933305</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-73.948584</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-230.472983</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Lasson Regression from scratch","metadata":{}},{"cell_type":"code","source":"class LassoRegression(Regression):\n    \"\"\"\n    Lasso Regression is one of the variance of the Linear Regression. This model doing the parameter learning \n    and regularization at the same time. This model uses the l1-regularization. \n    * Regularization will be one of the soluions to the Overfitting.\n    * Overfitting happens when the model has \"High Variance and low bias\". So, regularization adds a little bias to the model.\n    * This model will try to keep the balance between learning the parameters and the complexity of the model( tries to keep the parameter having small value and small degree of palinamial).\n    * The Regularization parameter(lamda) controls how severe  the regularization is. \n    * large lamda adds more bias , hence the Variance will go very small --> this may cause underfitting(Low bias and High Varinace).\n    * Lamda can be found by tial and error methos. \n    \"\"\"\n    def __init__(self, lamda, learning_rate, iteration):\n        \"\"\"\n        Define the hyperparameters we are going to use in this model.\n        :param lamda: Regularization factor.\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.regularization = l1_regularization(lamda)\n        super(LassoRegression, self).__init__(learning_rate, iteration, self.regularization)\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        return super(LassoRegression, self).train(X, y)\n    def predict(self, test_X):\n        \"\"\"\n        parma test_X: Value need to be predicted.\n        \"\"\"\n        return super(LassoRegression, self).predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.631589Z","iopub.execute_input":"2022-01-06T17:26:07.631896Z","iopub.status.idle":"2022-01-06T17:26:07.638854Z","shell.execute_reply.started":"2022-01-06T17:26:07.631866Z","shell.execute_reply":"2022-01-06T17:26:07.637945Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#define the parameters\nparam = {\n    \"lamda\" : 0.1,\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 100\n}\nprint(\"=\"*100)\nlinear_reg = LassoRegression(**param)\n\n# Train the model.\nlinear_reg.train(X, y) \n\n# Predict the values.\ny_pred = linear_reg.predict(X)\n\n#Root mean square error.\nscore = r2_score(y, y_pred)\nprint(\"The r2_score of the trained model\", score)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.720476Z","iopub.execute_input":"2022-01-06T17:26:07.720989Z","iopub.status.idle":"2022-01-06T17:26:07.815661Z","shell.execute_reply.started":"2022-01-06T17:26:07.720953Z","shell.execute_reply":"2022-01-06T17:26:07.814715Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"====================================================================================================\nThe Cost function for the iteration 10----->3916.9786338369713 :)\nThe Cost function for the iteration 20----->544.1076663280688 :)\nThe Cost function for the iteration 30----->122.58194356985209 :)\nThe Cost function for the iteration 40----->69.86966751035105 :)\nThe Cost function for the iteration 50----->63.27675960533016 :)\nThe Cost function for the iteration 60----->62.45108789980797 :)\nThe Cost function for the iteration 70----->62.34778632688957 :)\nThe Cost function for the iteration 80----->62.33491543483613 :)\nThe Cost function for the iteration 90----->62.33334215003843 :)\nThe Cost function for the iteration 100----->62.333164288048216 :)\nThe r2_score of the trained model 0.9999983118994883\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lasso Regression using skicit-learn","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\n\n# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:07.988280Z","iopub.execute_input":"2022-01-06T17:26:07.988625Z","iopub.status.idle":"2022-01-06T17:26:08.084069Z","shell.execute_reply.started":"2022-01-06T17:26:07.988591Z","shell.execute_reply":"2022-01-06T17:26:08.082943Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"====================================================================================================\nNumber of training data samples-----> 50000\nNumber of training features --------> 8\n","output_type":"stream"}]},{"cell_type":"code","source":"lasso_sklearn = Lasso()\nlasso_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = lasso_sklearn.predict(X)\nscore = r2_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"R2 score of the model is {}\".format(score))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:08.086108Z","iopub.execute_input":"2022-01-06T17:26:08.086479Z","iopub.status.idle":"2022-01-06T17:26:08.111239Z","shell.execute_reply.started":"2022-01-06T17:26:08.086410Z","shell.execute_reply":"2022-01-06T17:26:08.110281Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"====================================================================================================\nR2 score of the model is 0.999838932277606\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Conclution:\nOur model( from scratch) also works great as compared to skiti-learn model. Both the models are giving 0.99..% r2_socre which is good. :)","metadata":{}},{"cell_type":"markdown","source":"# Ridge Regression From scratch","metadata":{}},{"cell_type":"code","source":"class RidgeRegression(Regression):\n    \"\"\"\n    Ridge Regression is one of the variance of the Linear Regression. This model doing the parameter learning \n    and regularization at the same time. This model uses the l2-regularization. \n    This is very similar to the Lasso regression.\n    * Regularization will be one of the soluions to the Overfitting.\n    * Overfitting happens when the model has \"High Variance and low bias\". So, regularization adds a little bias to the model.\n    * This model will try to keep the balance between learning the parameters and the complexity of the model( tries to keep the parameter having small value and small degree of palinamial).\n    * The Regularization parameter(lamda) controls how severe  the regularization is. \n    * large lamda adds more bias , hence the Variance will go very small --> this may cause underfitting(Low bias and High Varinace).\n    * Lamda can be found by tial and error methos. \n    \"\"\"\n    def __init__(self, lamda, learning_rate, iteration):\n        \"\"\"\n        Define the hyperparameters we are going to use in this model.\n        :param lamda: Regularization factor.\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.regularization = l2_regularization(lamda)\n        super(RidgeRegression, self).__init__(learning_rate, iteration, self.regularization)\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        return super(RidgeRegression, self).train(X, y)\n    def predict(self, test_X):\n        \"\"\"\n        parma test_X: Value need to be predicted.\n        \"\"\"\n        return super(RidgeRegression, self).predict(test_X)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:08.327229Z","iopub.execute_input":"2022-01-06T17:26:08.327556Z","iopub.status.idle":"2022-01-06T17:26:08.334849Z","shell.execute_reply.started":"2022-01-06T17:26:08.327524Z","shell.execute_reply":"2022-01-06T17:26:08.333930Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#define the parameters\nparam = {\n    \"lamda\" : 0.1,\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 100\n}\nprint(\"=\"*100)\nlinear_reg = RidgeRegression(**param)\n\n# Train the model.\nlinear_reg.train(X, y) \n\n# Predict the values.\ny_pred = linear_reg.predict(X)\n\n#Root mean square error.\nscore = r2_score(y, y_pred)\nprint(\"The r2_score of the trained model\", score)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:08.446182Z","iopub.execute_input":"2022-01-06T17:26:08.446536Z","iopub.status.idle":"2022-01-06T17:26:08.543871Z","shell.execute_reply.started":"2022-01-06T17:26:08.446497Z","shell.execute_reply":"2022-01-06T17:26:08.542769Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"====================================================================================================\nThe Cost function for the iteration 10----->6377.807601960243 :)\nThe Cost function for the iteration 20----->4402.924400778202 :)\nThe Cost function for the iteration 30----->4245.366902415282 :)\nThe Cost function for the iteration 40----->4232.788553901341 :)\nThe Cost function for the iteration 50----->4231.7837191965 :)\nThe Cost function for the iteration 60----->4231.703394076125 :)\nThe Cost function for the iteration 70----->4231.696968783158 :)\nThe Cost function for the iteration 80----->4231.696454481357 :)\nThe Cost function for the iteration 90----->4231.696413288186 :)\nThe Cost function for the iteration 100----->4231.696409986675 :)\nThe r2_score of the trained model 0.9716460362886734\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Ridge Regression using scikit-learn","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\n\n# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:11.437404Z","iopub.execute_input":"2022-01-06T17:26:11.438105Z","iopub.status.idle":"2022-01-06T17:26:11.445713Z","shell.execute_reply.started":"2022-01-06T17:26:11.438051Z","shell.execute_reply":"2022-01-06T17:26:11.444732Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"====================================================================================================\nNumber of training data samples-----> 50000\nNumber of training features --------> 8\n","output_type":"stream"}]},{"cell_type":"code","source":"ridge_sklearn = Ridge()\nridge_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = ridge_sklearn.predict(X)\nscore = r2_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"R2 score of the model is {}\".format(score))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T17:26:11.447201Z","iopub.execute_input":"2022-01-06T17:26:11.447879Z","iopub.status.idle":"2022-01-06T17:26:11.480795Z","shell.execute_reply.started":"2022-01-06T17:26:11.447844Z","shell.execute_reply":"2022-01-06T17:26:11.479788Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"====================================================================================================\nR2 score of the model is 0.9999999995900196\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Supervised Machine Learning models scratch series....\nyou can also check....\n\n- 1) Linear Regression         ---> https://www.kaggle.com/ninjaac/linear-regression-from-scratch\n- 2) Lasso Regression          ---> https://www.kaggle.com/ninjaac/lasso-and-ridge-regression-from-scratch (Same Notebook you are looking now)\n- 3) Ridge Regression          ---> https://www.kaggle.com/ninjaac/lasso-and-ridge-regression-from-scratch (Same Notebook you are looking now)\n- 4) ElasticNet Regression     ---> https://www.kaggle.com/ninjaac/elasticnet-regression-from-scratch \n- 5) Polynomail Regression     ---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression-scratch \n- 5) PolynomailRidge Regression---> https://www.kaggle.com/ninjaac/polynomial-and-polynomialridge-regression-scratch \n- 6) KNN Classifier            ---> https://www.kaggle.com/ninjaac/knnclassifier-from-scratch ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}